


!pip install opencv-python


import cv2
import numpy as np
from ultralytics import YOLO


capture = cv2.VideoCapture(0)
face = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')
while True:
  ret, frame = capture.read()
  faces = face.detectMultiScale(
        cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY),
        scaleFactor=1.1,
        minNeighbors=5,
        minSize=(30, 30),
        )
  for (x, y, w, h) in faces:
    cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 255), 2)

  cv2.imshow('Face-Tracking(Haarcascade)',frame)
  if cv2.waitKey(10) == ord('q'):
    break
capture.release()
cv2.destroyAllWindows()


img = cv2.imread("smpl.jpg")
face = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')
faces = face.detectMultiScale(
    cv2.cvtColor(img, cv2.COLOR_BGR2GRAY),
    scaleFactor=1.1,
    minNeighbors=5,
    minSize=(5, 5),
)
for (x, y, w, h) in faces:
    cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 255), 2)
cv2.imshow('Face-Tracking(Haarcascade)',img)
cv2.waitKey(0)
cv2.destroyAllWindows()


from mtcnn.mtcnn import MTCNN
detector = MTCNN()
img = cv2.imread('test.jpg')
faces = detector.detect_faces(img)# result
#to draw faces on image
for result in faces:
    x, y, w, h = result['box']
    x1, y1 = x + w, y + h
    cv2.rectangle(img, (x, y), (x1, y1), (0, 0, 255), 2)


from mtcnn.mtcnn import MTCNN
capture = cv2.VideoCapture(0)
detector = MTCNN()
while True:
    ret, frame = capture.read()
    faces = detector.detect_faces(frame)
    for (x, y, w, h) in faces:
        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 255), 2)
        cv2.imshow('Face-Tracking',frame)
    if cv2.waitKey(10) == ord('q'):
        break
capture.release()
cv2.destroyAllWindows()


import cv2
import numpy as np
modelFile = "models/res10_300x300_ssd_iter_140000.caffemodel"
configFile = "models/deploy.prototxt.txt"
net = cv2.dnn.readNetFromCaffe(configFile, modelFile)
img = cv2.imread('test.jpg')
h, w = img.shape[:2]
blob = cv2.dnn.blobFromImage(cv2.resize(img, (300, 300)), 1.0,
(300, 300), (104.0, 117.0, 123.0))
net.setInput(blob)
faces = net.forward()
#to draw faces on image
for i in range(faces.shape[2]):
        confidence = faces[0, 0, i, 2]
        if confidence > 0.5:
            box = faces[0, 0, i, 3:7] * np.array([w, h, w, h])
            (x, y, x1, y1) = box.astype("int")
            cv2.rectangle(img, (x, y), (x1, y1), (0, 0, 255), 2)





!pip install ultralytics



!pip list



from ultralytics import YOLO


model = YOLO("yolov8n.pt")
model2 =YOLO("yolov8s.pt")
model3 =YOLO("yolov8m.pt")
model4 =YOLO("yolov8l.pt")
model5 =YOLO("yolov8x.pt")


model2.names


model3.names


model4.names


model5.names


img = cv2.imread("smpl.jpg")
result = model2.predict(source = img , classes = [0])
box = []
coords = []
for r in result:
    box = r.boxes
    coords = box.xyxy
    coords = coords.tolist()
for (x1, y1, x2, y2) in coords:
        cv2.rectangle(img, (int(x1), int(y1), int(x2), int(y2)), (0, 255, 255), 2)
# print(coords)
cv2.imshow('Face-Tracking(YOLO)',img)
cv2.waitKey(0)
cv2.destroyAllWindows()


# model.predict(source = "th.jpeg" ,save =False,conf =0.4,classes = [0])
model.names
# cv2.waitkey(0)
# cv2.destroAllWindows()


capture = cv2.VideoCapture(0)

while True:
    ret, frame = capture.read()
    if not ret:
        print("Failed to capture image")
        break
    result = model.predict(source = frame ,classes = [1])
    box = []
    coords = []
    for r in result:
        box = r.boxes
        coords = box.xyxy
        coords = coords.tolist()
    for (x1, y1, x2, y2) in coords:
        cv2.rectangle(frame, (int(x1), int(y1), int(x2), int(y2)), (0, 255, 255), 2)
    cv2.imshow("face-tracking(YOLO)",frame)
    key = cv2.waitKey(1)
    if key  == 27:
         break
capture.release()
cv2.destroyAllWindows()


capture = cv2.VideoCapture(0)

while True:
  ret, frame = capture.read()
  if not ret:
        print("Failed to capture image")
        break
  result = model.predict(source = frame ,show = True,save = False,conf = 0.4,classes = [1])
  # box = []
  # coords = []
  # for r in result:
  #   box = r.boxes
  #   coords = box.xyxy
  #   coords = coords.tolist()
  # for (x1, y1, x2, y2) in coords:
  #       cv2.rectangle(frame, (int(x1), int(y1), int(x2), int(y2)), (0, 255, 255), 2)
  # cv2.imshow("face-tracking(YOLO)",frame)
  if cv2.waitKey(0) & 0xFF == ord('q'):
      break
capture.release()
cv2.destroyAllWindows()


!pip install roboflow





from roboflow import Roboflow
rf = Roboflow(api_key="8sWv6lVaNzyeOi0IMq6c")
project = rf.workspace("hunau-csecl").project("face-detection-5w6i9")
version = project.version(1)
dataset = version.download("yolov8")


!nvidia-smi


results = model3.train(data="Face-Detection-1/data.yaml", epochs=10, imgsz=640 )


!pip list


new_model = YOLO("best.pt")


img = cv2.imread("smpl.jpg")
result = model.predict(source = img , classes = [0])
box = []
coords = []
for r in result:
    box = r.boxes
    coords = box.xyxy
    coords = coords.tolist()
for (x1, y1, x2, y2) in coords:
        cv2.rectangle(img, (int(x1), int(y1), int(x2), int(y2)), (0, 255, 255), 2)
# print(coords)
cv2.imshow('Face-Tracking(YOLO)',img)
cv2.waitKey(0)
cv2.destroyAllWindows()





!pip install face_recognition


import face_recognition





from PIL import Image
import face_recognition

# Load the jpg file into a numpy array
image = face_recognition.load_image_file("smpl.jpg")

# Find all the faces in the image using the default HOG-based model.
# This method is fairly accurate, but not as accurate as the CNN model and not GPU accelerated.
# See also: find_faces_in_picture_cnn.py
face_locations = face_recognition.face_locations(image)

print("I found {} face(s) in this photograph.".format(len(face_locations)))

for face_location in face_locations:

    # Print the location of each face in this image
    top, right, bottom, left = face_location
    print("A face is located at pixel location Top: {}, Left: {}, Bottom: {}, Right: {}".format(top, left, bottom, right))

    # You can access the actual face itself like this:
    face_image = image[top:bottom, left:right]
    pil_image = Image.fromarray(face_image)
    pil_image.show()





import cv2
import time

def calculate_iou(boxA, boxB):
    """
    Calculate the Intersection over Union (IoU) between two bounding boxes.
    """
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])
    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])

    interWidth = max(0, xB - xA)
    interHeight = max(0, yB - yA)
    interArea = interWidth * interHeight

    boxAArea = boxA[2] * boxA[3]
    boxBArea = boxB[2] * boxB[3]

    unionArea = boxAArea + boxBArea - interArea
    iou = interArea / float(unionArea) if unionArea != 0 else 0

    return iou


import glob
def load_encoding_images(images_path):
        # Load Images
        images_path = glob.glob(os.path.join(images_path, "*.*"))

        print("{} encoding images found.".format(len(images_path)))

        for img_path in images_path:
            img = cv2.imread(img_path)
            rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

            basename = os.path.basename(img_path)
            (filename, ext) = os.path.splitext(basename)
            # Get encoding
            img_encoding = face_recognition.face_encodings(rgb_img)[0]
            
            known_face_encodings.append(img_encoding)
            known_face_names.append(filename)
        print("Encoding images loaded")


def load_unkonown_image(img,name):
    rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    # Get encoding
    img_encoding = face_recognition.face_encodings(rgb_img)[0]        
    known_face_encodings.append(img_encoding)
    known_face_names.append(name)
    print('unknown person encoding image loaded')



import face_recognition
import cv2
import numpy as np
import time
import matplotlib.pyplot as plt
import os
# Initialize known face encodings and names
known_face_encodings = []
known_face_names = []

# Load known face encodings and names
# Assuming you have a function load_encoding_images() that loads the encodings and names
load_encoding_images("encoding")

# Initialize video capture
video_capture = cv2.VideoCapture(0)

# Variables to store face locations, encodings, and names
face_locations = []
face_encodings = []
face_names= []
process_this_frame = True

# Variables for latency measurement
total_time = 0
frame_count = 0
i = 1

face_matches = []

# Main loop for face detection and recognition
while True:
    ret, frame = video_capture.read()
    if not ret:
        print("Failed to grab frame")
        break

    if process_this_frame:
        small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)
        rgb_small_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)
        
        start = time.time()
        face_locations = face_recognition.face_locations(rgb_small_frame)
        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)
        end = time.time()
        elapsed_time = end - start
        total_time += elapsed_time
        print(f'len = {len(face_encodings)}')
        # face_matches = [True]*
        k = 0
        for face_encoding in face_encodings:
            
            start = time.time()
            matches = face_recognition.compare_faces(known_face_encodings, face_encoding)
            end = time.time()
            elapsed_time = end - start
            total_time += elapsed_time
            print(f'matches = {matches}')
            t = 0
            if c = 0:
                for b in matches:
                    if b == True:
                        match_index = t
                        print(f'first_match_index = {match_index}')
                        name = known_face_names[match_index]
                    face_names.append(name)
                    t+=1
            else:
                name = f'person{i}'
                print("inside else")
                for (top, right, bottom, left), name in zip(face_locations, face_names):
                    if name == f'person{i}' :
                        top *= 4
                        right *= 4
                        bottom *= 4
                        left *= 4
                        cropped_image = frame[top:bottom, left:right]
                        print(top, left,bottom,right)
                        load_unkonown_image(cropped_image,name)  
            face_names.append(name)
            k+=1
            i+=1                
    # for (top, right, bottom, left), name in zip(face_locations, face_names):
    #     if(name == f'person{i}' ):
    #         top *= 4
    #         right *= 4
    #         bottom *= 4
    #         left *= 4
    #         cropped_image = frame[top:bottom, left:right]
    #         print(top, left,bottom,right)
    #         load_unkonown_image(cropped_image,name)    
            # if known_face_encodings:  # Check if known_face_encodings is not empty
            #     face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)
            #     best_match_index = np.argmin(face_distances)
            #     if matches[best_match_index]:
            #         name = known_face_names[best_match_index]


    # process_this_frame = not process_this_frame
    

    for (top, right, bottom, left), name in zip(face_locations, face_names):
        top *= 4
        right *= 4
        bottom *= 4
        left *= 4

        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)
        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)
        font = cv2.FONT_HERSHEY_DUPLEX
        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)

    cv2.imshow('Video', frame)
    frame_count +=1

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break
    # if frame_count == 20:
    #     break

if frame_count > 0:
    average_latency = total_time / frame_count
    print(f'Average Latency: {average_latency:.6f} seconds')


video_capture.release()
cv2.destroyAllWindows()



import face_recognition
import cv2
import numpy as np
import time

# Initialize known face encodings and names
known_face_encodings = []
known_face_names = []

# Function to load known face encodings and names from a directory

# Function to load and recognize unknown faces
def recognize_unknown_faces(frame, face_locations):
    for (top, right, bottom, left) in face_locations:
        # Extract the face ROI (Region of Interest) from the frame
        face_image = frame[top:bottom, left:right]
        rgb_face_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)

        # Encode the face
        unknown_encoding = face_recognition.face_encodings(rgb_face_image)
        if len(unknown_encoding) > 0:
            unknown_encoding = unknown_encoding[0]

            # Compare with known encodings
            matches = face_recognition.compare_faces(known_face_encodings, unknown_encoding)
            name = "Unknown"

            # Find the best match
            if True in matches:
                first_match_index = matches.index(True)
                name = known_face_names[first_match_index]

            # Draw rectangle and label on the frame
            cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)
            cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)
            font = cv2.FONT_HERSHEY_DUPLEX
            cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)
    
    return frame

# Load known face encodings and names
load_encoding_images("encoding")

# Initialize video capture
video_capture = cv2.VideoCapture(0)

# Variables for latency measurement
total_time = 0
frame_count = 0

# Main loop for face detection and recognition
while True:
    ret, frame = video_capture.read()
    if not ret:
        print("Failed to grab frame")
        break

    # Resize frame of video to 1/4 size for faster face recognition processing
    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)
    rgb_small_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)
    
    # Measure time for face detection and recognition
    start = time.time()
    face_locations = face_recognition.face_locations(rgb_small_frame)
    frame = recognize_unknown_faces(frame, face_locations)
    end = time.time()
    elapsed_time = end - start
    total_time += elapsed_time
    frame_count += 1

    # Display the resulting image
    cv2.imshow('Video', frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Calculate and print average latency
if frame_count > 0:
    average_latency = total_time / frame_count
    print(f'Average Latency: {average_latency:.6f} seconds')

# Release handle to the webcam and close windows
video_capture.release()
cv2.destroyAllWindows()



import face_recognition
import cv2
import numpy as np
from collections import defaultdict

# Load known face encodings and names (replace with your actual implementation)
def load_encoding_images(folder):
    pass

known_face_encodings = []
known_face_names = []
load_encoding_images("images")

# Get a reference to webcam #0 (the default one)
video_capture = cv2.VideoCapture(0)

# Initialize variables
face_locations = []
face_encodings = []
face_names = []
process_this_frame = True

# Dictionary to hold face tracking data
face_trackers = defaultdict(dict)

while True:
    # Grab a single frame of video
    ret, frame = video_capture.read()
    if not ret:
        print("Failed to grab frame")
        break

    # Resize frame of video to 1/4 size for faster face recognition processing
    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)
    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)
    rgb_small_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)

    # Only process every other frame of video to save time
    if process_this_frame:
        # Locate faces in the current frame of video
        face_locations = face_recognition.face_locations(rgb_small_frame)
        
        if face_locations:  # Check if faces are detected
            # Encode faces found in the current frame of video
            face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)

            # Create a list to store names for each detected face
            face_names = []

            # Iterate through each detected face and track it
            for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):
                # Scale back up face locations since the frame we detected in was scaled to 1/4 size
                top *= 4
                right *= 4
                bottom *= 4
                left *= 4

                # Check if we already have a tracker for this face using its coordinates as ID
                face_id = (top, right, bottom, left)
                if face_id in face_trackers:
                    # Track this face using its unique ID
                    tracker = face_trackers[face_id]['tracker']
                    success, box = tracker.update(frame)
                    if success:
                        # Update the face location based on the tracker
                        (x, y, w, h) = [int(v) for v in box]
                        face_locations.append((y, x + w, y + h, x))

                else:
                    # Initialize new tracker for this face
                    tracker = cv2.TrackerKCF_create()
                    tracker.init(frame, (left, top, right - left, bottom - top))
                    face_trackers[face_id]['tracker'] = tracker

                # Perform face recognition if there are known faces
                if known_face_encodings:
                    # Compare the detected face with the known faces
                    matches = face_recognition.compare_faces(known_face_encodings, face_encoding)
                    name = "Unknown"

                    # If a match is found, use the known face with the smallest distance to the new face
                    face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)
                    best_match_index = np.argmin(face_distances)
                    if matches[best_match_index]:
                        name = known_face_names[best_match_index]

                    face_names.append(name)
                else:
                    face_names.append("Unknown")

                # Update the tracked face data in face_trackers dictionary
                face_trackers[face_id]['name'] = name
                face_trackers[face_id]['last_seen'] = frame

        else:
            face_names = []

    process_this_frame = not process_this_frame

    # Display the results
    for (top, right, bottom, left), name in zip(face_locations, face_names):
        # Draw a box around the face
        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)

        # Draw a label with a name below the face
        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)
        font = cv2.FONT_HERSHEY_DUPLEX
        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)

    # Display the resulting image
    cv2.imshow('Video', frame)

    # Hit 'q' on the keyboard to quit!
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release handle to the webcam
video_capture.release()
cv2.destroyAllWindows()



import face_recognition
import cv2
import numpy as np

# Load known face encodings and names (replace with your actual implementation)
def load_encoding_images(folder):
    pass

known_face_encodings = []
known_face_names = []
load_encoding_images("images")

# Get a reference to webcam #0 (the default one)
video_capture = cv2.VideoCapture(0)

# Initialize variables
face_locations = []
face_encodings = []
face_names = ["Puneet"]
process_this_frame = True

while True:
    # Grab a single frame of video
    ret, frame = video_capture.read()
    if not ret:
        print("Failed to grab frame")
        break

    # Resize frame of video to 1/4 size for faster face recognition processing
    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)
    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)
    rgb_small_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)

    # Only process every other frame of video to save time
    if process_this_frame:
        # Locate faces in the current frame of video
        face_locations = face_recognition.face_locations(rgb_small_frame)
        
        if face_locations:  # Check if faces are detected
            # Encode faces found in the current frame of video
            face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)

            face_names = []
            for face_encoding in face_encodings:
                if known_face_encodings:  # Check if there are known faces to compare against
                    # Compare the detected face(s) with the known faces
                    matches = face_recognition.compare_faces(known_face_encodings, face_encoding)
                    name = "Unknown"

                    # If a match is found, use the known face with the smallest distance to the new face
                    face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)
                    best_match_index = np.argmin(face_distances)
                    if matches[best_match_index]:
                        name = known_face_names[best_match_index]

                    face_names.append(name)
                else:
                    face_names.append("Unknown")
        else:
            face_names = []

    process_this_frame = not process_this_frame

    # Display the results
    for (top, right, bottom, left), name in zip(face_locations, face_names):
        # Scale back up face locations since the frame we detected in was scaled to 1/4 size
        top *= 4
        right *= 4
        bottom *= 4
        left *= 4

        # Draw a box around the face
        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)

        # Draw a label with a name below the face
        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)
        font = cv2.FONT_HERSHEY_DUPLEX
        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)

    # Display the resulting image
    cv2.imshow('Video', frame)

    # Hit 'q' on the keyboard to quit!
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release handle to the webcam
video_capture.release()
cv2.destroyAllWindows()






import time
capture = cv2.VideoCapture("VID_20240616211418.mp4")
face = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')
# start_time = time.time()
frame_count = 0
total_time = 0
while True:
    ret, frame = capture.read()
    start = time.time()
    faces = face.detectMultiScale(
        cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY),
        scaleFactor=1.1,
        minNeighbors=5,
        minSize=(30, 30),
        )
    end = time.time()
    elapsed_time = end - start
    total_time += elapsed_time
    frame_count += 1
    for (x, y, w, h) in faces:
        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 255), 2)

    cv2.imshow('Face-Tracking(Haarcascade)',frame)
    if cv2.waitKey(10) == ord('q'):
        break
    if frame_count == 20:
        break
average_latency = total_time / frame_count
print(f'Average Latency: {average_latency:.6f} seconds')
capture.release()
cv2.destroyAllWindows()


import time
capture = cv2.VideoCapture("VID_20240616211418.mp4")
face = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')
# start_time = time.time()
frame_count = 0
total_time = 0
faces = []
while True:
    ret, frame = capture.read()
    start = time.time()
    faces = face.detectMultiScale(
        cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY),
        scaleFactor=1.1,
        minNeighbors=5,
        minSize=(30, 30),
        )
    end = time.time()
    elapsed_time = end - start
    total_time += elapsed_time
    frame_count += 1
    for (x, y, w, h) in faces:
        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 255), 2)

    cv2.imshow('Face-Tracking(Haarcascade)',frame)
    if cv2.waitKey(10) == ord('q'):
        break
    if frame_count == 20:
        break


for (x, y, w, h) in faces:
        iou =  
average_latency = total_time / frame_count
print(f'Average Latency: {average_latency:.6f} seconds')
capture.release()
cv2.destroyAllWindows()


import face_recognition
import cv2
import numpy as np
import time

def calculate_iou(boxA, boxB):
    """
    Calculate the Intersection over Union (IoU) between two bounding boxes.
    """
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])
    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])

    interWidth = max(0, xB - xA)
    interHeight = max(0, yB - yA)
    interArea = interWidth * interHeight

    boxAArea = boxA[2] * boxA[3]
    boxBArea = boxB[2] * boxB[3]

    unionArea = boxAArea + boxBArea - interArea
    iou = interArea / float(unionArea) if unionArea != 0 else 0

    return iou

def load_encoding_images(folder):
    # Dummy function for loading face encodings
    # Replace this with the actual implementation
    pass

# Initialize known face encodings and names
known_face_encodings = []
known_face_names = []

# Load known face encodings and names
load_encoding_images("encoding")

# Initialize video capture
video_capture = cv2.VideoCapture("VID_20240616211418.mp4")

# Variables to store face locations, encodings, and names
face_locations = []
face_encodings = []
face_names = []
process_this_frame = True

# Variables for latency measurement
total_time = 0
frame_count = 0

# Placeholder for ground truth face data (example data, replace with actual)
ground_truth_faces = [
    [(659, 383, 586, 694)],    # (top, left, height, width)
    [(615, 371, 561, 706)],
    [(499, 306, 559, 752)],
    [(549, 344, 527, 732)],
    [(669, 358, 608, 719)],
    [(664, 382, 613, 721)],
    [(487, 291, 530, 752)],
    [(469, 265, 744, 744)],
    [(667, 378, 576, 754)],
    [(659, 344, 570, 747)],
    [(647, 382, 582, 750)],
    [(480, 279, 726, 726)],
    [(673, 377, 576, 749)],
    [(659, 344, 570, 747)],
    [(636, 379, 570, 827)],
    [(673, 367, 571, 867)],
    [(522, 321, 739, 749)],
    [(670, 379, 577, 755)],
    [(471, 267, 728, 745)],
    [(583, 356, 581, 721)]
]

predicted_boxes = []

# Main loop for face detection and recognition
while True:
    ret, frame = video_capture.read()
    if not ret:
        print("Failed to grab frame")
        break

    if process_this_frame:
        small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)
        rgb_small_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)
        
        start = time.time()
        face_locations = face_recognition.face_locations(rgb_small_frame)
        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)

        face_names = []
        face_confidences = []
        for face_encoding in face_encodings:
            matches = face_recognition.compare_faces(known_face_encodings, face_encoding)
            name = "Unknown"
            confidence = 0.0

            if known_face_encodings:  # Check if known_face_encodings is not empty
                face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)
                best_match_index = np.argmin(face_distances)
                confidence = 1 - face_distances[best_match_index]  # Higher distance means lower confidence
                if matches[best_match_index]:
                    name = known_face_names[best_match_index]

            face_names.append(name)
            face_confidences.append(confidence)
        
        end = time.time()
        elapsed_time = end - start
        total_time += elapsed_time
        frame_count += 1

        # Calculate and print IoU for each detected face
        frame_ground_truth_faces = ground_truth_faces[frame_count - 1] if frame_count - 1 < len(ground_truth_faces) else []
        for detected_face, name, confidence in zip(face_locations, face_names, face_confidences):
            # Convert detected face to (x, y, width, height) format
            detected_face = (detected_face[3], detected_face[0], detected_face[1] - detected_face[3], detected_face[2] - detected_face[0])
            absolute_coordinates = (detected_face[0] * 4, detected_face[1] * 4, (detected_face[0] + detected_face[2]) * 4, (detected_face[1] + detected_face[3]) * 4)
            cv2.rectangle(frame, (absolute_coordinates[0], absolute_coordinates[1]), (absolute_coordinates[2], absolute_coordinates[3]), (0, 0, 255), 2)
            
            for ground_truth_face in frame_ground_truth_faces:
                # Scale down ground truth face locations by 4
                scaled_gt_face = (ground_truth_face[0] // 4, ground_truth_face[1] // 4, ground_truth_face[2] // 4, ground_truth_face[3] // 4)
                iou = calculate_iou(detected_face, scaled_gt_face)
                print(f'Frame {frame_count} - IoU: {iou:.6f}')
            
            # Add detected face to predicted_boxes
            predicted_boxes.append(f"face {confidence:.2f} {absolute_coordinates[0]} {absolute_coordinates[1]} {absolute_coordinates[2]} {absolute_coordinates[3]} {absolute_coordinates}")

    process_this_frame = not process_this_frame

    for (top, right, bottom, left), name in zip(face_locations, face_names):
        top *= 4
        right *= 4
        bottom *= 4
        left *= 4

        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)
        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)
        font = cv2.FONT_HERSHEY_DUPLEX
        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)

    cv2.imshow('Video', frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break
    if frame_count == 20:
        break

if frame_count > 0:
    average_latency = total_time / frame_count
    print(f'Average Latency: {average_latency:.6f} seconds')
else:
    print('No frames were processed.')

# Write the predicted bounding boxes to a file
with open('predicted_boxes.txt', 'w') as f:
    for box in predicted_boxes:
        f.write(f"{box}\n")

video_capture.release()
cv2.destroyAllWindows()



import cv2
import time

# Load video capture and face detector
capture = cv2.VideoCapture("VID_20240616211418.mp4")
face = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')

# Initialize variables
total_time = 0
frame_count = 0

# Placeholder for ground truth face data
# Replace this with actual ground truth data for each frame
ground_truth_faces = [
    [(659,383,1245,1077)],
[(615,371,1176,1077)],
[(499,306,1033,1058)],
[(549,344,1096,1076)],[(669,358,1242,1077)],[(664,382,1247,1077)],
[(487,291,1017,1043)],[(469,265,992,1009)],[(667,378,1243,1077)],[(659,344,1229,1077)],
[(647,382,1226,1077)],
[(480,279,1005,1029)],[(673,377,1249,1077)],[(659,344,1229,1077)],
[(636,379,1206,1077)],[(673,367,1244,1077)],
[(522,321,1060,1070)],[(670,379,1247,1077)],[(471,267,995,1012)],
[(583,356,1138,1077)]
]

    # [(x1, y1, w1, h1), (x2, y2, w2, h2), ...]
    # Add your actual ground truth face data here


while True:
    ret, frame = capture.read()
    
    if not ret:
        break
    
    start = time.time()
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = face.detectMultiScale(
        gray,
        scaleFactor=1.1,
        minNeighbors=5,
        minSize=(30, 30)
    )
    end = time.time()
    
    elapsed_time = end - start
    total_time += elapsed_time
    frame_count += 1
    
    frame_ground_truth_faces = ground_truth_faces[frame_count - 1] if frame_count - 1 < len(ground_truth_faces) else []
    
    for detected_face in faces:
        cv2.rectangle(frame, (detected_face[0], detected_face[1]), (detected_face[0]+detected_face[2], detected_face[1]+detected_face[3]), (0, 255, 255), 2)
        
        for ground_truth_face in frame_ground_truth_faces:
            iou = calculate_iou(detected_face, ground_truth_face)
            print(f'Frame {frame_count} - IoU: {iou:.6f}')
    
    cv2.imshow('Face-Tracking(Haarcascade)', frame)
    
    if cv2.waitKey(10) == ord('q'):
        break
    if frame_count == 20:
        break

if frame_count > 0:
    average_latency = total_time / frame_count
    print(f'Average Latency: {average_latency:.6f} seconds')
else:
    print('No frames were processed.')

capture.release()
cv2.destroyAllWindows()



import face_recognition
import cv2
import numpy as np
import time

def calculate_iou(boxA, boxB):
    """
    Calculate the Intersection over Union (IoU) between two bounding boxes.
    """
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])
    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])

    interWidth = max(0, xB - xA)
    interHeight = max(0, yB - yA)
    interArea = interWidth * interHeight

    boxAArea = boxA[2] * boxA[3]
    boxBArea = boxB[2] * boxB[3]

    unionArea = boxAArea + boxBArea - interArea
    iou = interArea / float(unionArea) if unionArea != 0 else 0

    return iou

def load_encoding_images(folder):
    # Dummy function for loading face encodings
    # Replace this with the actual implementation
    pass

# Initialize known face encodings and names
known_face_encodings = []
known_face_names = []

# Load known face encodings and names
load_encoding_images("encoding")

# Initialize video capture
video_capture = cv2.VideoCapture("VID_20240616211418.mp4")

# Variables to store face locations, encodings, and names
face_locations = []
face_encodings = []
face_names = []
process_this_frame = True

# Variables for latency measurement
total_time = 0
frame_count = 0

# Placeholder for ground truth face data (example data, replace with actual)
ground_truth_faces = [
    [(659, 383, 586, 694)],    # (top, left, height, width)
    [(615, 371, 561, 706)],
    [(499, 306, 559, 752)],
    [(549, 344, 527, 732)],
    [(669, 358, 608, 719)],
    [(664, 382, 613, 721)],
    [(487, 291, 530, 752)],
    [(469, 265, 744, 744)],
    [(667, 378, 576, 754)],
    [(659, 344, 570, 747)],
    [(647, 382, 582, 750)],
    [(480, 279, 726, 726)],
    [(673, 377, 576, 749)],
    [(659, 344, 570, 747)],
    [(636, 379, 570, 827)],
    [(673, 367, 571, 867)],
    [(522, 321, 739, 749)],
    [(670, 379, 577, 755)],
    [(471, 267, 728, 745)],
    [(583, 356, 581, 721)]
]

predicted_boxes = []

# Main loop for face detection and recognition
while True:
    ret, frame = video_capture.read()
    if not ret:
        print("Failed to grab frame")
        break

    if process_this_frame:
        small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)
        rgb_small_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)
        
        start = time.time()
        face_locations = face_recognition.face_locations(rgb_small_frame)
        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)

        face_names = []
        face_confidences = []
        for face_encoding in face_encodings:
            matches = face_recognition.compare_faces(known_face_encodings, face_encoding)
            name = "Unknown"
            confidence = 0.0

            if known_face_encodings:  # Check if known_face_encodings is not empty
                face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)
                best_match_index = np.argmin(face_distances)
                confidence = 1 - face_distances[best_match_index]  # Higher distance means lower confidence
                if matches[best_match_index]:
                    name = known_face_names[best_match_index]

            face_names.append(name)
            face_confidences.append(confidence)
        
        end = time.time()
        elapsed_time = end - start
        total_time += elapsed_time
        frame_count += 1

        # Calculate and print IoU for each detected face
        frame_ground_truth_faces = ground_truth_faces[frame_count - 1] if frame_count - 1 < len(ground_truth_faces) else []
        for detected_face, name, confidence in zip(face_locations, face_names, face_confidences):
            # Convert detected face to (x, y, width, height) format
            detected_face = (detected_face[3], detected_face[0], detected_face[1] - detected_face[3], detected_face[2] - detected_face[0])
            absolute_coordinates = (detected_face[0] * 4, detected_face[1] * 4, (detected_face[0] + detected_face[2]) * 4, (detected_face[1] + detected_face[3]) * 4)
            cv2.rectangle(frame, (absolute_coordinates[0], absolute_coordinates[1]), (absolute_coordinates[2], absolute_coordinates[3]), (0, 0, 255), 2)
            
            for ground_truth_face in frame_ground_truth_faces:
                # Scale down ground truth face locations by 4
                scaled_gt_face = (ground_truth_face[0] // 4, ground_truth_face[1] // 4, ground_truth_face[2] // 4, ground_truth_face[3] // 4)
                iou = calculate_iou(detected_face, scaled_gt_face)
                print(f'Frame {frame_count} - IoU: {iou:.6f}')
            
            # Add detected face to predicted_boxes
            predicted_boxes.append(f"face {confidence:.2f} {absolute_coordinates[0]} {absolute_coordinates[1]} {absolute_coordinates[2]} {absolute_coordinates[3]} {absolute_coordinates}")

    process_this_frame = not process_this_frame

    for (top, right, bottom, left), name in zip(face_locations, face_names):
        top *= 4
        right *= 4
        bottom *= 4
        left *= 4

        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)
        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)
        font = cv2.FONT_HERSHEY_DUPLEX
        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)

    cv2.imshow('Video', frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break
    if frame_count == 20:
        break")

video_capture.release()
cv2.destroyAllWindows()



import face_recognition
import cv2
import numpy as np
import time

def calculate_iou(boxA, boxB):
    """
    Calculate the Intersection over Union (IoU) between two bounding boxes.
    """
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])
    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])

    interWidth = max(0, xB - xA)
    interHeight = max(0, yB - yA)
    interArea = interWidth * interHeight

    boxAArea = boxA[2] * boxA[3]
    boxBArea = boxB[2] * boxB[3]

    unionArea = boxAArea + boxBArea - interArea
    iou = interArea / float(unionArea) if unionArea != 0 else 0

    return iou

def load_encoding_images(folder):
    # Dummy function for loading face encodings
    # Replace this with the actual implementation
    pass

# Initialize known face encodings and names
known_face_encodings = []
known_face_names = []

# Load known face encodings and names
load_encoding_images("encoding")

# Initialize video capture
video_capture = cv2.VideoCapture("VID_20240616211418.mp4")

# Variables to store face locations, encodings, and names
face_locations = []
face_encodings = []
face_names = []
process_this_frame = True

# Variables for latency measurement
total_time = 0
frame_count = 0

# Placeholder for ground truth face data (example data, replace with actual)
ground_truth_faces = [
    [(659, 383, 586, 694)],    # (top, left, height, width)
    [(615, 371, 561, 706)],
    [(499, 306, 559, 752)],
    [(549, 344, 527, 732)],
    [(669, 358, 608, 719)],
    [(664, 382, 613, 721)],
    [(487, 291, 530, 752)],
    [(469, 265, 744, 744)],
    [(667, 378, 576, 754)],
    [(659, 344, 570, 747)],
    [(647, 382, 582, 750)],
    [(480, 279, 726, 726)],
    [(673, 377, 576, 749)],
    [(659, 344, 570, 747)],
    [(636, 379, 570, 827)],
    [(673, 367, 571, 867)],
    [(522, 321, 739, 749)],
    [(670, 379, 577, 755)],
    [(471, 267, 728, 745)],
    [(583, 356, 581, 721)]
]

predicted_boxes = []

# Main loop for face detection and recognition
while True:
    ret, frame = video_capture.read()
    if not ret:
        print("Failed to grab frame")
        break

    if process_this_frame:
        small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)
        rgb_small_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)
        
        start = time.time()
        face_locations = face_recognition.face_locations(rgb_small_frame)
        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)

        face_names = []
        face_confidences = []
        for face_encoding in face_encodings:
            matches = face_recognition.compare_faces(known_face_encodings, face_encoding)
            name = "Unknown"
            confidence = 0.0

            if known_face_encodings:  # Check if known_face_encodings is not empty
                face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)
                best_match_index = np.argmin(face_distances)
                confidence = 1 - face_distances[best_match_index]  # Higher distance means lower confidence
                if matches[best_match_index]:
                    name = known_face_names[best_match_index]

            face_names.append(name)
            face_confidences.append(confidence)
        
        end = time.time()
        elapsed_time = end - start
        total_time += elapsed_time
        frame_count += 1

        # Calculate and print IoU for each detected face
        frame_ground_truth_faces = ground_truth_faces[frame_count - 1] if frame_count - 1 < len(ground_truth_faces) else []
        for detected_face, name, confidence in zip(face_locations, face_names, face_confidences):
            # Convert detected face to (x, y, width, height) format
            detected_face = (detected_face[3], detected_face[0], detected_face[1] - detected_face[3], detected_face[2] - detected_face[0])
            absolute_coordinates = (detected_face[0] * 4, detected_face[1] * 4, (detected_face[0] + detected_face[2]) * 4, (detected_face[1] + detected_face[3]) * 4)
            cv2.rectangle(frame, (absolute_coordinates[0], absolute_coordinates[1]), (absolute_coordinates[2], absolute_coordinates[3]), (0, 0, 255), 2)
            
            for ground_truth_face in frame_ground_truth_faces:
                # Scale down ground truth face locations by 4
                scaled_gt_face = (ground_truth_face[0] // 4, ground_truth_face[1] // 4, ground_truth_face[2] // 4, ground_truth_face[3] // 4)
                iou = calculate_iou(detected_face, scaled_gt_face)
                print(f'Frame {frame_count} - IoU: {iou:.6f}')
            
            # Add detected face to predicted_boxes
            predicted_boxes.append(f"face {confidence:.2f} {absolute_coordinates[0]} {absolute_coordinates[1]} {absolute_coordinates[2]} {absolute_coordinates[3]} {absolute_coordinates}")

    process_this_frame = not process_this_frame

    for (top, right, bottom, left), name in zip(face_locations, face_names):
        top *= 4
        right *= 4
        bottom *= 4
        left *= 4

        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)
        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)
        font = cv2.FONT_HERSHEY_DUPLEX
        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)

    cv2.imshow('Video', frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break
    if frame_count == 20:
        break

if frame_count > 0:
    average_latency = total_time / frame_count
    print(f'Average Latency: {average_latency:.6f} seconds')
else:
    print('No frames were processed.')

# Write the predicted bounding boxes to a file
with open('predicted_boxes.txt', 'w') as f:
    for box in predicted_boxes:
        f.write(f"{box}\n")

video_capture.release()
cv2.destroyAllWindows()



import face_recognition
import cv2
import numpy as np
import time
import glob
import os

def calculate_iou(boxA, boxB):
    """
    Calculate the Intersection over Union (IoU) between two bounding boxes.
    """
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])
    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])

    interWidth = max(0, xB - xA)
    interHeight = max(0, yB - yA)
    interArea = interWidth * interHeight

    boxAArea = boxA[2] * boxA[3]
    boxBArea = boxB[2] * boxB[3]

    unionArea = boxAArea + boxBArea - interArea
    iou = interArea / float(unionArea) if unionArea != 0 else 0

    return iou

# Initialize known face encodings and names
known_face_encodings = []
known_face_names = []

# Load known face encodings and names
load_encoding_images("encoding")

# Initialize video capture
video_capture = cv2.VideoCapture("VID_20240616211418.mp4")

# Variables to store face locations, encodings, and names
face_locations = []
face_encodings = []
face_names = []
process_this_frame = True

# Variables for latency measurement
total_time = 0
frame_count = 0

# Placeholder for ground truth face data (example data, replace with actual)
ground_truth_faces = [
    [(659,383,1245,1077)],
[(615,371,1176,1077)],
[(499,306,1033,1058)],
[(549,344,1096,1076)],[(669,358,1242,1077)],[(664,382,1247,1077)],
[(487,291,1017,1043)],[(469,265,992,1009)],[(667,378,1243,1077)],[(659,344,1229,1077)],
[(647,382,1226,1077)],
[(480,279,1005,1029)],[(673,377,1249,1077)],[(659,344,1229,1077)],
[(636,379,1206,1077)],[(673,367,1244,1077)],
[(522,321,1060,1070)],[(670,379,1247,1077)],[(471,267,995,1012)],
[(583,356,1138,1077)]
]
gg = []
ggg = []
# Main loop for face detection and recognition
while True:
    ret, frame = video_capture.read()
    if not ret:
        print("Failed to grab frame")
        break

    if process_this_frame:
        small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)
        rgb_small_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)
        
        start = time.time()
        face_locations = face_recognition.face_locations(rgb_small_frame)
        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)

        face_names = []
        for face_encoding in face_encodings:
            matches = face_recognition.compare_faces(known_face_encodings, face_encoding)
            name = "Unknown"

            if known_face_encodings:  # Check if known_face_encodings is not empty
                face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)
                best_match_index = np.argmin(face_distances)
                if matches[best_match_index]:
                    name = known_face_names[best_match_index]

            face_names.append(name)
        
        end = time.time()
        elapsed_time = end - start
        total_time += elapsed_time
        frame_count += 1

        # Calculate and print IoU for each detected face
        frame_ground_truth_faces = ground_truth_faces[frame_count - 1] if frame_count - 1 < len(ground_truth_faces) else []
        for detected_face in face_locations:
            # Scale back up face locations since the frame we detected in was scaled to 1/4 size
            detected_face = (detected_face[3]*4, detected_face[0]*4, (detected_face[1] - detected_face[3])*4, (detected_face[2] - detected_face[0])*4)
            cv2.rectangle(frame, (detected_face[0], detected_face[1]), (detected_face[0]+detected_face[2], detected_face[1]+detected_face[3]), (0, 0, 255), 2)
            
            for ground_truth_face in frame_ground_truth_faces:
                iou = calculate_iou(detected_face, ground_truth_face)
                print(f'Frame {frame_count} - IoU: {iou:.6f}')
                
                # print(f'[({face_locations*4})]')

    process_this_frame = not process_this_frame

    for (top, right, bottom, left), name in zip(face_locations, face_names):
        top *= 4
        right *= 4
        bottom *= 4
        left *= 4
        gg = [(top,right,bottom,left)]
        ggg.append(gg)
        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)
        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)
        font = cv2.FONT_HERSHEY_DUPLEX
        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)

    cv2.imshow('Video', frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break
    if frame_count == 20:
        break

if frame_count > 0:
    average_latency = total_time / frame_count
    print(f'Average Latency: {average_latency:.6f} seconds')
else:
    print('No frames were processed.')
print(ggg)
video_capture.release()
cv2.destroyAllWindows()



import face_recognition
import cv2
import numpy as np
import time
import glob
import os

def calculate_iou(boxA, boxB):
    """
    Calculate the Intersection over Union (IoU) between two bounding boxes.
    """
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])
    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])

    interWidth = max(0, xB - xA)
    interHeight = max(0, yB - yA)
    interArea = interWidth * interHeight

    boxAArea = boxA[2] * boxA[3]
    boxBArea = boxB[2] * boxB[3]

    unionArea = boxAArea + boxBArea - interArea
    iou = interArea / float(unionArea) if unionArea != 0 else 0

    return iou

# Initialize known face encodings and names
known_face_encodings = []
known_face_names = []

# Load known face encodings and names
load_encoding_images("encoding")

# Initialize video capture
video_capture = cv2.VideoCapture(0)

# Variables to store face locations, encodings, and names
face_locations = []
face_encodings = []
face_names = []
process_this_frame = True

# Variables for latency measurement
total_time = 0
frame_count = 0


gg = []
# Main loop for face detection and recognition
while True:
    ret, frame = video_capture.read()
    if not ret:
        print("Failed to grab frame")
        break

    if process_this_frame:
        small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)
        rgb_small_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)
        
        start = time.time()
        face_locations = face_recognition.face_locations(rgb_small_frame)
        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)

        face_names = []
        for face_encoding in face_encodings:
            matches = face_recognition.compare_faces(known_face_encodings, face_encoding)
            name = "Unknown"

            if known_face_encodings:  # Check if known_face_encodings is not empty
                face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)
                best_match_index = np.argmin(face_distances)
                if matches[best_match_index]:
                    name = known_face_names[best_match_index]

            face_names.append(name)
        
        end = time.time()
    
    process_this_frame = not process_this_frame

    for (top, right, bottom, left), name in zip(face_locations, face_names):
        top *= 4
        right *= 4
        bottom *= 4
        left *= 4

        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)
        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)
        font = cv2.FONT_HERSHEY_DUPLEX
        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)

    cv2.imshow('Video', frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break
    if frame_count == 20:
        break

if frame_count > 0:
    average_latency = total_time / frame_count
    print(f'Average Latency: {average_latency:.6f} seconds')
video_capture.release()
cv2.destroyAllWindows()



import face_recognition
import cv2
import numpy as np
import time
import glob
import os

def calculate_iou(boxA, boxB):
    """
    Calculate the Intersection over Union (IoU) between two bounding boxes.
    """
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])
    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])

    interWidth = max(0, xB - xA)
    interHeight = max(0, yB - yA)
    interArea = interWidth * interHeight

    boxAArea = boxA[2] * boxA[3]
    boxBArea = boxB[2] * boxB[3]

    unionArea = boxAArea + boxBArea - interArea
    iou = interArea / float(unionArea) if unionArea != 0 else 0

    return iou

# Initialize known face encodings and names
known_face_encodings = []
known_face_names = []

# Load known face encodings and names
# Assuming you have a function load_encoding_images() that loads the encodings and names
load_encoding_images("encoding")

# Initialize video capture
video_capture = cv2.VideoCapture("VID_20240616211418.mp4")

# Variables to store face locations, encodings, and names
face_locations = []
face_encodings = []
face_names = []
process_this_frame = True

# Variables for latency measurement
total_time = 0
frame_count = 0


predicted_boxes = []

# Main loop for face detection and recognition
while True:
    ret, frame = video_capture.read()
    if not ret:
        print("Failed to grab frame")
        break

    if process_this_frame:
        small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)
        rgb_small_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)
        
        start = time.time()
        face_locations = face_recognition.face_locations(rgb_small_frame)
        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)

        face_names = []
        for face_encoding in face_encodings:
            matches = face_recognition.compare_faces(known_face_encodings, face_encoding)
            name = "Unknown"

            if known_face_encodings:  # Check if known_face_encodings is not empty
                face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)
                best_match_index = np.argmin(face_distances)
                if matches[best_match_index]:
                    name = known_face_names[best_match_index]

            face_names.append(name)
        
        end = time.time()
        elapsed_time = end - start
        total_time += elapsed_time
        frame_count += 1

        # Calculate and print IoU for each detected face
        frame_ground_truth_faces = ground_truth_faces[frame_count - 1] if frame_count - 1 < len(ground_truth_faces) else []
        for detected_face in face_locations:
            # Convert detected face to (x, y, width, height) format
            detected_face = (detected_face[3], detected_face[0], detected_face[1] - detected_face[3], detected_face[2] - detected_face[0])
            cv2.rectangle(frame, (detected_face[0] * 4, detected_face[1] * 4), ((detected_face[0] + detected_face[2]) * 4, (detected_face[1] + detected_face[3]) * 4), (0, 0, 255), 2)
            
            for ground_truth_face in frame_ground_truth_faces:
                # Scale down ground truth face locations by 4
                scaled_gt_face = (ground_truth_face[0] // 4, ground_truth_face[1] // 4, ground_truth_face[2] // 4, ground_truth_face[3] // 4)
                iou = calculate_iou(detected_face, scaled_gt_face)
                print(f'Frame {frame_count} - IoU: {iou:.6f}')
                gg.append(face_locations)
                print(f'[({face_locations})]')
            
            # Add detected face to predicted_boxes
            predicted_boxes.append(f"face {detected_face[0] * 4} {detected_face[1] * 4} {(detected_face[0] + detected_face[2]) * 4} {(detected_face[1] + detected_face[3]) * 4}")

    process_this_frame = not process_this_frame

    for (top, right, bottom, left), name in zip(face_locations, face_names):
        top *= 4
        right *= 4
        bottom *= 4
        left *= 4

        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)
        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)
        font = cv2.FONT_HERSHEY_DUPLEX
        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)

    cv2.imshow('Video', frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break
    if frame_count == 20:
        break

if frame_count > 0:
    average_latency = total_time / frame_count
    print(f'Average Latency: {average_latency:.6f} seconds')
else:
    print('No frames were processed.')
print(gg)

# Write the predicted bounding boxes to a file
with open('predicted_boxes.txt', 'w') as f:
    for box in predicted_boxes:
        f.write(f"{box}\n")

video_capture.release()
cv2.destroyAllWindows()



import face_recognition
import cv2
import numpy as np
import time

def calculate_iou(boxA, boxB):
    """
    Calculate the Intersection over Union (IoU) between two bounding boxes.
    """
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])
    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])

    interWidth = max(0, xB - xA)
    interHeight = max(0, yB - yA)
    interArea = interWidth * interHeight

    boxAArea = boxA[2] * boxA[3]
    boxBArea = boxB[2] * boxB[3]

    unionArea = boxAArea + boxBArea - interArea
    iou = interArea / float(unionArea) if unionArea != 0 else 0

    return iou

def load_encoding_images(folder):
    # Dummy function for loading face encodings
    # Replace this with the actual implementation
    pass

# Initialize known face encodings and names
known_face_encodings = []
known_face_names = []

# Load known face encodings and names
load_encoding_images("encoding")

# Initialize video capture
video_capture = cv2.VideoCapture("VID_20240616211418.mp4")

# Variables to store face locations, encodings, and names
face_locations = []
face_encodings = []
face_names = []
process_this_frame = True

# Variables for latency measurement
total_time = 0
frame_count = 0

# Placeholder for ground truth face data (example data, replace with actual)
ground_truth_faces = [
    [(659, 383, 586, 694)],    # (top, left, height, width)
    [(615, 371, 561, 706)],
    [(499, 306, 559, 752)],
    [(549, 344, 527, 732)],
    [(669, 358, 608, 719)],
    [(664, 382, 613, 721)],
    [(487, 291, 530, 752)],
    [(469, 265, 744, 744)],
    [(667, 378, 576, 754)],
    [(659, 344, 570, 747)],
    [(647, 382, 582, 750)],
    [(480, 279, 726, 726)],
    [(673, 377, 576, 749)],
    [(659, 344, 570, 747)],
    [(636, 379, 570, 827)],
    [(673, 367, 571, 867)],
    [(522, 321, 739, 749)],
    [(670, 379, 577, 755)],
    [(471, 267, 728, 745)],
    [(583, 356, 581, 721)]
]

predicted_boxes = []

# Main loop for face detection and recognition
while True:
    ret, frame = video_capture.read()
    if not ret:
        print("Failed to grab frame")
        break

    if process_this_frame:
        small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)
        rgb_small_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)
        
        start = time.time()
        face_locations = face_recognition.face_locations(rgb_small_frame)
        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)

        face_names = []
        face_confidences = []
        for face_encoding in face_encodings:
            matches = face_recognition.compare_faces(known_face_encodings, face_encoding)
            name = "Unknown"
            confidence = 0.0

            if known_face_encodings:  # Check if known_face_encodings is not empty
                face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)
                best_match_index = np.argmin(face_distances)
                confidence = 1 - face_distances[best_match_index]  # Higher distance means lower confidence
                if matches[best_match_index]:
                    name = known_face_names[best_match_index]

            face_names.append(name)
            face_confidences.append(confidence)
        
        end = time.time()
        elapsed_time = end - start
        total_time += elapsed_time
        frame_count += 1

        # Calculate and print IoU for each detected face
        frame_ground_truth_faces = ground_truth_faces[frame_count - 1] if frame_count - 1 < len(ground_truth_faces) else []
        for detected_face, name, confidence in zip(face_locations, face_names, face_confidences):
            # Convert detected face to (x, y, width, height) format
            detected_face = (detected_face[3], detected_face[0], detected_face[1] - detected_face[3], detected_face[2] - detected_face[0])
            absolute_coordinates = (detected_face[0] * 4, detected_face[1] * 4, (detected_face[0] + detected_face[2]) * 4, (detected_face[1] + detected_face[3]) * 4)
            cv2.rectangle(frame, (absolute_coordinates[0], absolute_coordinates[1]), (absolute_coordinates[2], absolute_coordinates[3]), (0, 0, 255), 2)
            
            for ground_truth_face in frame_ground_truth_faces:
                # Scale down ground truth face locations by 4
                scaled_gt_face = (ground_truth_face[0] // 4, ground_truth_face[1] // 4, ground_truth_face[2] // 4, ground_truth_face[3] // 4)
                iou = calculate_iou(detected_face, scaled_gt_face)
                print(f'Frame {frame_count} - IoU: {iou:.6f}')
            
            # Add detected face to predicted_boxes
            predicted_boxes.append(f"face {confidence:.2f} {absolute_coordinates[0]} {absolute_coordinates[1]} {absolute_coordinates[2]} {absolute_coordinates[3]} {absolute_coordinates}")

    process_this_frame = not process_this_frame

    for (top, right, bottom, left), name in zip(face_locations, face_names):
        top *= 4
        right *= 4
        bottom *= 4
        left *= 4

        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)
        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)
        font = cv2.FONT_HERSHEY_DUPLEX
        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)

    cv2.imshow('Video', frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break
    if frame_count == 20:
        break

if frame_count > 0:
    average_latency = total_time / frame_count
    print(f'Average Latency: {average_latency:.6f} seconds')
else:
    print('No frames were processed.')

# Write the predicted bounding boxes to a file
with open('predicted_boxes.txt', 'w') as f:
    for box in predicted_boxes:
        f.write(f"{box}\n")

video_capture.release()
cv2.destroyAllWindows() 


import face_recognition
import cv2
import numpy as np
import time

# Function to calculate Intersection over Union (IoU) between two bounding boxes
def calculate_iou(boxA, boxB):
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])
    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])

    interWidth = max(0, xB - xA)
    interHeight = max(0, yB - yA)
    interArea = interWidth * interHeight

    boxAArea = boxA[2] * boxA[3]
    boxBArea = boxB[2] * boxB[3]

    unionArea = boxAArea + boxBArea - interArea
    iou = interArea / float(unionArea) if unionArea != 0 else 0

    return iou

# Load known face encodings and names (you need to implement this function)
def load_encoding_images(folder_path):
    # Implement your loading logic here
    pass

# Initialize known face encodings and names
known_face_encodings = []
known_face_names = []

# Load known face encodings and names
load_encoding_images("encoding")  # Implement this function

# Initialize video capture
video_capture = cv2.VideoCapture("VID_20240616211418.mp4")

# Placeholder for ground truth face data (example data, replace with actual)
ground_truth_faces = [
  (659, 383, 586, 694),    # (top, left, height, width)
    (615, 371, 561, 706),
    (499, 306, 559, 752),
    (549, 344, 527, 732),
    (669, 358, 608, 719),
    (664, 382, 613, 721),
    (487, 291, 530, 752),
    (469, 265, 744, 744),
    (667, 378, 576, 754),
    (659, 344, 570, 747),
    (647, 382, 582, 750),
    (480, 279, 726, 726),
    (673, 377, 576, 749),
    (659, 344, 570, 747),
    (636, 379, 570, 827),
    (673, 367, 571, 867),
    (522, 321, 739, 749),
    (670, 379, 577, 755),
    (471, 267, 728, 745),
    (583, 356, 581, 721)
]

# Main loop for face detection and recognition
while True:
    ret, frame = video_capture.read()
    if not ret:
        print("Failed to grab frame")
        break

    # Resize frame to speed up face detection
    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)
    rgb_small_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)

    # Detect faces in the frame
    face_locations = face_recognition.face_locations(rgb_small_frame)
    face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)

    # Process each detected face
    for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):
        # Scale coordinates back to original frame size
        top *= 4
        right *= 4
        bottom *= 4
        left *= 4

        # Draw rectangle around the detected face (predicted)
        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)

    # Draw rectangles for ground truth faces
    # Draw rectangles for ground truth faces
    for gt_face in ground_truth_faces:
        if len(gt_face) == 4:  # Ensure each ground truth face annotation has 4 elements (xmin, ymin, width, height)
            xmin, ymin, width, height = gt_face
            cv2.rectangle(frame, (xmin, ymin), (xmin + width, ymin + height), (0, 255, 0), 2)
        else:
            print(f"Invalid ground truth face annotation: {gt_face}")

# Display the resulting frame
    cv2.imshow('Video', frame)


    # Exit loop if 'q' is pressed
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release video capture and close all windows
video_capture.release()
cv2.destroyAllWindows()



!python -m pip install mediapipe



model_path = 'face_detector.task'

import cv2
import time

# Load video capture and face detector
capture = cv2.VideoCapture("VID_20240616211418.mp4")
frame_count = 1
# fps = capture.get(cv2.cv.CV_CAP_PROP_FPS)
while True:
    ret, frame = capture.read()
    
    if not ret:
        break

    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)
    face_detector_result = detector.detect_for_video(mp_image, frame_count)
    start = time.time()
    end = time.time()
    elapsed_time = end - start
    total_time += elapsed_time
    frame_count += 1
    
    # frame_ground_truth_faces = ground_truth_faces[frame_count - 1] if frame_count - 1 < len(ground_truth_faces) else []
    
    # for detected_face in faces:
    #     cv2.rectangle(frame, (detected_face[0], detected_face[1]), (detected_face[0]+detected_face[2], detected_face[1]+detected_face[3]), (0, 255, 255), 2)
        
    #     for ground_truth_face in frame_ground_truth_faces:
    #         iou = calculate_iou(detected_face, ground_truth_face)
    #         print(f'Frame {frame_count} - IoU: {iou:.6f}')
    
    cv2.imshow('Face-Tracking(Haarcascade)', frame)
    
    if cv2.waitKey(10) == ord('q'):
        break
    if frame_count == 20:
        break

if frame_count > 0:
    average_latency = total_time / frame_count
    print(f'Average Latency: {average_latency:.6f} seconds')
else:
    print('No frames were processed.')

capture.release()
cv2.destroyAllWindows()



cv2.CV_CAP_PROP_FPS()


import mediapipe as mp
import numpy as np
import cv2

cap = cv2.VideoCapture(0)

facmesh = mp.solutions.face_mesh
face = facmesh.FaceMesh(static_image_mode=True, min_tracking_confidence=0.6, min_detection_confidence=0.6)
draw = mp.solutions.drawing_utils

while True:

	_, frm = cap.read()
	print(frm.shape)
	break
	rgb = cv2.cvtColor(frm, cv2.COLOR_BGR2RGB)

	op = face.process(rgb)
	if op.multi_face_landmarks:
		for i in op.multi_face_landmarks:
			print(i.landmark[0].y*480)
			draw.draw_landmarks(frm, i, facmesh.FACEMESH_CONTOURS, landmark_drawing_spec=draw.DrawingSpec(color=(0, 255, 255), circle_radius=1))


	cv2.imshow("window", frm)

	if cv2.waitKey(1) == 27:
		break
cap.release()
cv2.destroyAllWindows()


import cv2
import mediapipe as mp
mp_face_detection = mp.solutions.face_detection
mp_drawing = mp.solutions.drawing_utils

# For webcam input:
cap = cv2.VideoCapture(0)
with mp_face_detection.FaceDetection(
    model_selection=0, min_detection_confidence=0.5) as face_detection:
  while cap.isOpened():
    success, image = cap.read()
    if not success:
      print("Ignoring empty camera frame.")
      # If loading a video, use 'break' instead of 'continue'.
      continue

    # To improve performance, optionally mark the image as not writeable to
    # pass by reference.
    image.flags.writeable = False
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    results = face_detection.process(image)

    # Draw the face detection annotations on the image.
    image.flags.writeable = True
    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
    if results.detections:
      for detection in results.detections:
        mp_drawing.draw_detection(image, detection)
    # Flip the image horizontally for a selfie-view display.
    cv2.imshow('MediaPipe Face Detection', cv2.flip(image, 1))
    if cv2.waitKey(5) & 0xFF == 27:
        break
cap.release()
cv2.destroyAllWindows()


!pip install inference inference-sdk


!pip install inference-sdk




from inference_sdk import InferenceHTTPClient

CLIENT = InferenceHTTPClient(
    api_url="https://detect.roboflow.com",
    api_key="8sWv6lVaNzyeOi0IMq6c"
)

result = CLIENT.infer("Messi1.webp", model_id="face-recognition-gffkh/1")



result


from inference_sdk import InferenceHTTPClient
import cv2
import numpy as np

# Initialize the InferenceHTTPClient
CLIENT = InferenceHTTPClient(
    api_url="https://detect.roboflow.com",
    api_key="8sWv6lVaNzyeOi0IMq6c"
)

# Initialize video capture from the webcam
video_capture = cv2.VideoCapture(0)  # Use 0 for default webcam

while True:
    # Capture frame-by-frame
    ret, frame = video_capture.read()
    if not ret:
        print("Failed to grab frame")
        break

    # Perform face recognition inference on the current frame
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    result = CLIENT.infer(rgb_frame, model_id="face-recognition-gffkh/1")

    # Check if inference result contains predictions
    if 'predictions' in result:
        predictions = result['predictions']
        image_width = result['image']['width']
        image_height = result['image']['height']

        # Draw bounding boxes on the frame
        for prediction in predictions:
            x = int(prediction['x'])
            y = int(prediction['y'])
            width = int(prediction['width'])
            height = int(prediction['height'])
            confidence = prediction['confidence']

            # Convert relative coordinates to absolute coordinates
            left = x
            top = y
            right = x + width
            bottom = y + height

            # Draw bounding box
            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)

            # Display label with confidence
            label = f"{prediction['class']} {confidence:.2f}"
            cv2.putText(frame, label, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    # Display the resulting frame
    cv2.imshow('Video', frame)

    # Exit loop if 'q' key is pressed
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release video capture and close all windows
video_capture.release()
cv2.destroyAllWindows()



from inference_sdk import InferenceHTTPClient
import cv2
import time

# Initialize the InferenceHTTPClient
CLIENT = InferenceHTTPClient(
    api_url="https://detect.roboflow.com",
    api_key="8sWv6lVaNzyeOi0IMq6c"
)

# Initialize video capture from the webcam
video_capture = cv2.VideoCapture(0)  # Use 0 for default webcam

while True:
    # Capture frame-by-frame and measure start time
    
    ret, frame = video_capture.read()
    if not ret:
        print("Failed to grab frame")
        break

    # Perform face recognition inference on the current frame
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    start_time = time.time()
    result = CLIENT.infer(rgb_frame, model_id="face-recognition-gffkh/1")

    # Measure end time after receiving inference results
    end_time = time.time()
    latency = end_time - start_time
    print(f"Latency: {latency:.4f} seconds")

    # Check if inference result contains predictions
    if 'predictions' in result:
        predictions = result['predictions']
        image_width = result['image']['width']
        image_height = result['image']['height']

        # Draw bounding boxes on the frame
        for prediction in predictions:
            x = int(prediction['x'] * image_width)
            y = int(prediction['y'] * image_height)
            width = int(prediction['width'] * image_width)
            height = int(prediction['height'] * image_height)
            confidence = prediction['confidence']

            # Convert relative coordinates to absolute coordinates
            left = x
            top = y
            right = x + width
            bottom = y + height

            # Draw bounding box
            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)

            # Display label with confidence
            label = f"{prediction['class']} {confidence:.2f}"
            cv2.putText(frame, label, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    # Display the resulting frame
    cv2.imshow('Video', frame)

    # Exit loop if 'q' key is pressed
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release video capture and close all windows
video_capture.release()
cv2.destroyAllWindows()



from inference_sdk import InferenceHTTPClient
import cv2
import numpy as np
import os
import time

# Initialize the InferenceHTTPClient with the model_id
CLIENT = InferenceHTTPClient(
    api_url="https://detect.roboflow.com",
    api_key="8sWv6lVaNzyeOi0IMq6c",
)

# Function to calculate Intersection over Union (IoU)
def calculate_iou(boxA, boxB):
    """
    Calculate the Intersection over Union (IoU) between two bounding boxes.
    """
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])
    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])

    interWidth = max(0, xB - xA)
    interHeight = max(0, yB - yA)
    interArea = interWidth * interHeight

    boxAArea = boxA[2] * boxA[3]
    boxBArea = boxB[2] * boxB[3]

    unionArea = boxAArea + boxBArea - interArea
    iou = interArea / float(unionArea) if unionArea != 0 else 0

    return iou

# Directory containing images
images_dir = "ground-images"

# List to store latency values
latencies = []

# Ground truth bounding boxes for each image (example data, replace with actual)
ground_truth_faces = [[(659, 383, 586, 694)],
    [(615, 371, 561, 706)],
    [(499, 306, 534, 725)],
    [(549, 344, 547, 732)],
    [(669, 358, 573, 720)],
    [(664, 382, 580, 681)],
    [(487, 291, 526, 752)],
    [(469, 265, 523, 744)],
    [(667, 378, 576, 700)],
    [(659, 344, 570, 720)],
    [(647, 382, 580, 701)],
    [(480, 279, 526, 750)],
    [(673, 377, 577, 700)],
    [(659, 344, 570, 720)],
    [(636, 379, 571, 698)],
    [(673, 367, 573, 703)],
    [(522, 321, 538, 749)],
    [(670, 379, 577, 700)],
    [(471, 267, 528, 745)],
   [(583, 356, 555, 720)]
                    ]

# Process each image in the directory
for filename in os.listdir(images_dir):
    if filename.endswith(".jpg") or filename.endswith(".png") or filename.endswith(".jpeg"):
        # Read the image
        image_path = os.path.join(images_dir, filename)
        image = cv2.imread(image_path)

        # Perform inference on the image
        start_time = time.time()

        # Directly infer the image using CLIENT.infer()
        result = CLIENT.infer(image_path, model_id="face-recognition-gffkh/1")

        end_time = time.time()
        latency = end_time - start_time
        latencies.append(latency)

        # Check if inference result contains predictions
        if 'predictions' in result:
            predictions = result['predictions']

            # Draw bounding boxes on the image and calculate IoU
            for prediction in predictions:
                # Extract prediction bounding box
                pred_box = (
                    int(prediction['x']),
                    int(prediction['y']),
                    int(prediction['width']),
                    int(prediction['height'])
                )

                # Find the corresponding ground truth box if available
                gt_box = None
                for gt_face in ground_truth_faces:
                    # Assuming the model predicts only one face per image
                    if len(gt_face) > 0:
                        gt_box = (
                            int(gt_face[0][0]),  # left
                            int(gt_face[0][1]),  # top
                            int(gt_face[0][0] + gt_face[0][3]),  # right
                            int(gt_face[0][1] + gt_face[0][2])   # bottom
                        )
                        ground_truth_faces.remove(gt_face)
                        break

                if gt_box is not None:
                    # Calculate IoU
                    iou = calculate_iou(pred_box, gt_box)
                    print(f"Image: {filename}, IoU: {iou:.4f}")

                    # Draw bounding box and label on the image
                    left, top, width, height = pred_box
                    right = left + width
                    bottom = top + height
                    cv2.rectangle(image, (left, top), (right, bottom), (0, 255, 0), 2)
                    label = f"{prediction['class']} {prediction['confidence']:.2f}"
                    cv2.putText(image, label, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

            # Display the image with bounding boxes
            cv2.imshow('Image with Bounding Boxes', image)
            cv2.waitKey(0)

# Calculate average latency
if latencies:
    average_latency = sum(latencies) / len(latencies)
    print(f'Average Latency: {average_latency:.6f} seconds')
else:
    print('No images were processed.')



from inference_sdk import InferenceHTTPClient
import cv2
import numpy as np
import os
import time

# Initialize the InferenceHTTPClient with the model_id
CLIENT = InferenceHTTPClient(
    api_url="https://detect.roboflow.com",
    api_key="8sWv6lVaNzyeOi0IMq6c",
)

# Function to calculate Intersection over Union (IoU)
def calculate_iou(boxA, boxB):
    """
    Calculate the Intersection over Union (IoU) between two bounding boxes.
    """
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])
    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])

    interWidth = max(0, xB - xA)
    interHeight = max(0, yB - yA)
    interArea = interWidth * interHeight

    boxAArea = boxA[2] * boxA[3]
    boxBArea = boxB[2] * boxB[3]

    unionArea = boxAArea + boxBArea - interArea
    iou = interArea / float(unionArea) if unionArea != 0 else 0

    return iou

# Directory containing images
images_dir = "ground-images"

# List to store latency values
latencies = []

# Ground truth bounding boxes for each image (example data, replace with actual)
ground_truth_faces = [
    [(659//4, 383//4, 586//4, 694//4)],    # (top, left, height, width)
    [(615//4, 371//4, 561//4, 706//4)],
    [(499//4, 306//4, 559//4, 752//4)],
    [(549//4, 344//4, 527//4, 732//4)],
    [(669//4, 358//4, 608//4, 719//4)],
    [(664//4, 382//4, 613//4, 721//4)],
    [(487//4, 291//4, 530//4, 752//4)],
    [(469//4, 265//4, 744//4, 744//4)],
    [(667//4, 378//4, 576//4, 754//4)],
    [(659//4, 344//4, 570//4, 747//4)],
    [(647//4, 382//4, 582//4, 750//4)],
    [(480//4, 279//4, 726//4, 726//4)],
    [(673//4, 377//4, 576//4, 749//4)],
    [(659//4, 344//4, 570//4, 747//4)],
    [(636//4, 379//4, 570//4, 827//4)],
    [(673//4, 367//4, 571//4, 867//4)],
    [(522//4, 321//4, 739//4, 749//4)],
    [(670//4, 379//4, 577//4, 755//4)],
    [(471//4, 267//4, 728//4, 745//4)],
    [(583//4, 356//4, 581//4, 721//4)]
]

# Process each image in the directory
for filename in os.listdir(images_dir):
    if filename.endswith(".jpg") or filename.endswith(".png") or filename.endswith(".jpeg"):
        # Read the image
        image_path = os.path.join(images_dir, filename)
        image = cv2.imread(image_path)

        # Resize the image to 1/4 of its original size
        resized_image = cv2.resize(image, (image.shape[1] // 4, image.shape[0] // 4))

        # Perform inference on the resized image
        start_time = time.time()

        # Directly infer the resized image using CLIENT.infer()
        result = CLIENT.infer(resized_image, model_id="face-recognition-gffkh/1")

        end_time = time.time()
        latency = end_time - start_time
        latencies.append(latency)

        # Check if inference result contains predictions
        if 'predictions' in result:
            predictions = result['predictions']

            # Draw bounding boxes on the original image and calculate IoU
            for prediction in predictions:
                # Scale the prediction bounding box back to the original size
                pred_box = (
                    int(prediction['x'] * 4),
                    int(prediction['y'] * 4),
                    int(prediction['width'] * 4),
                    int(prediction['height'] * 4)
                )

                # Find the corresponding ground truth box if available
                gt_box = None
                for gt_face in ground_truth_faces:
                    # Assuming the model predicts only one face per image
                    if len(gt_face) > 0:
                        gt_box = (
                            int(gt_face[0][1]),  # left
                            int(gt_face[0][0]),  # top
                            int(gt_face[0][3]),  # width
                            int(gt_face[0][2])   # height
                        )
                        ground_truth_faces.remove(gt_face)
                        break

                if gt_box is not None:
                    # Calculate IoU
                    iou = calculate_iou(pred_box, gt_box)
                    print(f"Image: {filename}, IoU: {iou:.4f}")

                    # Draw bounding box and label on the original image
                    left, top, width, height = pred_box
                    right = left + width
                    bottom = top + height
                    cv2.rectangle(image, (left, top), (right, bottom), (0, 255, 0), 2)
                    label = f"{prediction['class']} {prediction['confidence']:.2f}"
                    cv2.putText(image, label, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

            # Display the original image with bounding boxes
            cv2.imshow('Image with Bounding Boxes', image)
            cv2.waitKey(0)

# Calculate average latency
if latencies:
    average_latency = sum(latencies) / len(latencies)
    print(f'Average Latency: {average_latency:.6f} seconds')
else:
    print('No images were processed.')




